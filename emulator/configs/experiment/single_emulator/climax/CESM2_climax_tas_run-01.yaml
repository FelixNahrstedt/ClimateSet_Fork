# @package _global_
# to execute this experiment run:
# python run.py experiment=example

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: unet.yaml # put the desired model name here
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /datamodule: dummy.yaml # standard datamodule configurations

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# can also be accessed by loggers
name: "CESM2_unet_tas_run-01"

seed: 22201

trainer:
  min_epochs: 1
  max_epochs: 50

model:
  loss_function: "climax_lon_lat_rmse"
  monitor: "val/llrmse_climax"
  finetune: False
  pretrained_run_id: null # eg "3u0ys0d5"
  pretrained_ckpt_dir: null # e.g "/network/scratch/c/charlotte.lange/causalpaca/emulator/emulator/3u0ys0d5/checkpoints/epoch=0-step=750.ckpt"

datamodule: # overwrite what stuff to train on
    # more selection like climate models and scenarios + splits should go here
    # ...
   in_var_ids: ['BC', 'CO2', 'SO2', 'CH4']
   out_var_ids: ['tas']
   train_historical_years: "1850-2010"
   train_models:  ["CESM2"]
   seq_length: 120
   seq_to_seq: True # determine the task setting
   batch_size: 4
   channels_last: True
   eval_batch_size: 4

logger:
  wandb:
    tags: ["single_emulator", "unet", "CESM2", "tas", "run1"] # set your tags here
