# Adapted from https://github.com/RolnickLab/climart

# specify here default training configuration
defaults:
  - _self_
  - trainer: debug.yaml
  - model: unet.yaml
  #- datamodule: dummy.yaml

  - callbacks: default.yaml  # or use wandb.yaml for wandb suppport
  - logger: wandb # set logger here or use command line (e.g. `python run.py logger=wandb`)

  # modes are special collections of config options for different purposes, e.g. debugging
  - mode: default.yaml

  # experiment configs allow for version control of specific configurations
  # for example, use them to store best hyperparameters for each combination of model and datamodule
  - experiment: null

  # config for hyperparameter optimization
  - hparams_search: null

  # optional local config for machine/user specific settings
  - optional local: default.yaml

  # enable color logging
  #- override hydra/hydra_logging: colorlog
  # - override hydra/job_logging: colorlog
  # default optimizer is Adam
  #- override optimizer@model.optimizer: adam.yaml

model:
  loss_function: "climax_lon_lat_rmse"
  monitor: "val/llrmse_climax"
  #finetune: True
  #pretrained_run_id: "26ilvxpg"
  #pretrained_ckpt_dir: "/home/mila/c/charlotte.lange/scratch/causalpaca/emulator/emulator/26ilvxpg/checkpoints/epoch=19-step=15000.ckpt" #/home/mila/c/charlotte.lange/scratch/causalpaca/emulator/emulator/${pretrained_run_id}/checkpoints/
 

# TODO make a yaml file for data specs
# USING DUMMY FOR NOW
datamodule:
   _target_: emulator.src.datamodules.dummy_datamodule.DummyDataModule
   lon: 32
   lat: 33
   in_var_ids: ['BC', 'CO2', 'SO2', 'CH4']
   out_var_ids: ['pr', 'tas']
   seq_len: 3
   seq_to_seq: True
   num_levels: 1
   batch_size: 4
   channels_last: True
   size: 5000
   eval_batch_size: 4
   
# TODO make a yaml file for normalization
# normalizer:
#   _target_: climart.data_transform.normalization.Normalizer
#   input_normalization: "z"
#   output_normalization: "z"
#   spatial_normalization_in:  False
#   spatial_normalization_out: False
#   log_scaling: False
#   data_dir: ${datamodule.data_dir}
#   verbose: ${verbose}
# path to original working directory
# hydra hijacks working directory by changing it to the new log directory
# so it's useful to have this path as a special variable
# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
work_dir: ${hydra:runtime.cwd}  # {oc.env:ENV_VAR} allows to get environment variable ENV_VAR

# TODO consider making yaml files for the metrics?
#val_metric: "val/${target_var_id:heating_rate, ${datamodule.target_type}}/rmse"

# path to checkpoints
ckpt_dir: ${work_dir}/checkpoints/

# path for logging
log_dir: ${work_dir}/logs/

# pretty print config at the start of the run using Rich library
print_config: True

# disable python warnings if they annoy you
ignore_warnings: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on metric specified in checkpoint callback
test_after_training: True

# Upload config file to wandb cloud?
save_config_to_wandb: True

# Verbose?
verbose: True

# seed for random number generators in pytorch, numpy and python.random
seed: 42

# name of the run, should be used along with experiment mode
name: "default"
